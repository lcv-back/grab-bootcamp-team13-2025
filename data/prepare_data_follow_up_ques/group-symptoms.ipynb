{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11726445,"sourceType":"datasetVersion","datasetId":7361010}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install -U sentence-transformers scikit-learn tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:21:31.324219Z","iopub.execute_input":"2025-05-08T04:21:31.324482Z","iopub.status.idle":"2025-05-08T04:22:53.495596Z","shell.execute_reply.started":"2025-05-08T04:21:31.324463Z","shell.execute_reply":"2025-05-08T04:22:53.494639Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ---------- 0. Cài thư viện (chỉ 1 lần) -------------------------\n\n\n# ---------- 1. Cấu hình đường dẫn ------------------------------\nINPUT_JSON  = \"/kaggle/input/symptoms-name/symptom_names.json\"  # <— đổi cho khớp\nOUTPUT_JSON = \"/kaggle/working/symptom_groups_semantic_90.json\"\n\nMODEL_NAME  = \"sentence-transformers/all-MiniLM-L6-v2\"   # nhẹ (~60 MB)\nBATCH_SIZE  = 128     # giảm nếu RAM hạn chế\nSIM_THRESH  = 0.90    # “≈90 % giống nhau”\n\n# ---------- 2. Đọc danh sách triệu chứng -----------------------\nimport json, numpy as np, os\nfrom tqdm.auto import tqdm\n\nwith open(INPUT_JSON, encoding=\"utf-8\") as f:\n    symptoms = json.load(f)\n\nprint(f\"🔎 Loaded {len(symptoms):,} symptom strings\")\n\n# ---------- 3. Mã hoá câu (sentence embedding) -----------------\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(MODEL_NAME)\nembeddings = model.encode(\n    symptoms,\n    batch_size=BATCH_SIZE,\n    show_progress_bar=True,\n    convert_to_numpy=True,\n    normalize_embeddings=True      # cosine-sim = dot-prod\n)\n\n# ---------- 4. Gom cụm bằng DBSCAN -----------------------------\nfrom sklearn.cluster import DBSCAN\n\neps = 1 - SIM_THRESH   # cosine-dist = 1 − cosine-sim\nclusterer = DBSCAN(eps=eps, metric=\"cosine\", min_samples=1)\nlabels = clusterer.fit_predict(embeddings)\n\n# ---------- 5. Xây dict nhóm ↔ alias ---------------------------\nfrom collections import defaultdict\n\ngroups = defaultdict(list)\nfor label, symptom in zip(labels, symptoms):\n    groups[label].append(symptom)\n\ndef pick_canonical(variants):\n    # lấy biến thể ngắn nhất (tuỳ chỉnh theo nhu cầu)\n    return min(variants, key=len)\n\nresult = [\n    {\"canonical\": pick_canonical(v), \"aliases\": sorted(set(v))}\n    for v in groups.values()   # bỏ lẻ 1 nếu muốn\n]\n\nprint(f\"✅ Formed {len(result):,} multi-alias groups\")\n\n# ---------- 6. Ghi file JSON kết quả ---------------------------\nwith open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n    json.dump(result, f, ensure_ascii=False, indent=2)\n\nprint(f\"📂 Saved → {OUTPUT_JSON}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T06:05:23.808040Z","iopub.execute_input":"2025-05-08T06:05:23.808193Z","iopub.status.idle":"2025-05-08T06:06:05.089638Z","shell.execute_reply.started":"2025-05-08T06:05:23.808178Z","shell.execute_reply":"2025-05-08T06:06:05.088837Z"}},"outputs":[{"name":"stdout","text":"🔎 Loaded 22,765 symptom strings\n","output_type":"stream"},{"name":"stderr","text":"2025-05-08 06:05:37.298962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746684337.492135      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746684337.547047      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a32773aa94e4b63aaceb533442b6109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"507f43cda5a442b89b43eafb3909925b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5abd358dd6e413f8736638d6d9b457b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07c205956fb24cefbba4eb2d2335c976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2b96c7f33c44368e39806290fa33e3"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e57a36f810794fe6a8649aad37e0b777"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ae21325f07a41adb03797ad66554624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b6ba500246841809ad1fb886bb61fff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02588805896c4eea9ebac933a1f3c113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4209164b9dac4be1b6f1285be4203dc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6ee92c7d4e45d0acf86ef695e868b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f1f68c5006449ba8d4d5c1a4c9a1b92"}},"metadata":{}},{"name":"stdout","text":"✅ Formed 16,075 multi-alias groups\n📂 Saved → /kaggle/working/symptom_groups_semantic_90.json\n","output_type":"stream"}],"execution_count":1}]}